{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLLAMA - LLM\n",
    "* https://medium.com/techcraft-chronicles/how-to-use-llama-3-2-with-ollama-using-python-e816162e9184\n",
    "\n",
    "* https://medium.com/@tapanbabbar/how-to-run-llama-3-2-vision-on-ollama-a-game-changer-for-edge-ai-80cb0e8d8928\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. **Sunlight enters Earth's atmosphere**: When sunlight enters our atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2).\n",
      "2. **Scattering occurs**: These gas molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths.\n",
      "3. **Blue light is scattered**: The blue light is scattered by the gas molecules in all directions, reaching our eyes from every part of the sky.\n",
      "4. **Red light travels straight**: Meanwhile, the longer wavelengths of light, such as red and orange, continue to travel in a more direct path, reaching our eyes from the direction of the sun.\n",
      "\n",
      "As a result, when we look up at the sky, we see primarily the blue light that has been scattered by the gas molecules, giving the sky its blue appearance. The color of the sky can also change depending on the time of day, atmospheric conditions, and other factors.\n",
      "\n",
      "Other colors of light can be seen during sunrise and sunset due to a phenomenon called Mie scattering, which occurs when light interacts with particles in the atmosphere, such as dust, water droplets, and pollutants.\n",
      "The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. **Sunlight enters Earth's atmosphere**: When sunlight enters our atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2).\n",
      "2. **Scattering occurs**: These gas molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths.\n",
      "3. **Blue light is scattered**: The blue light is scattered by the gas molecules in all directions, reaching our eyes from every part of the sky.\n",
      "4. **Red light travels straight**: Meanwhile, the longer wavelengths of light, such as red and orange, continue to travel in a more direct path, reaching our eyes from the direction of the sun.\n",
      "\n",
      "As a result, when we look up at the sky, we see primarily the blue light that has been scattered by the gas molecules, giving the sky its blue appearance. The color of the sky can also change depending on the time of day, atmospheric conditions, and other factors.\n",
      "\n",
      "Other colors of light can be seen during sunrise and sunset due to a phenomenon called Mie scattering, which occurs when light interacts with particles in the atmosphere, such as dust, water droplets, and pollutants.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2:3b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(question):\n",
    "    print(\"ASK LLM.....\")\n",
    "    response = chat(\n",
    "        model='llama3.2:3b',\n",
    "        #'llama3.2-vision',\n",
    "        #'llama3',\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question ,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASK LLM.....\n",
      "Here's one: Elephants have a highly developed brain and are considered one of the smartest animals on Earth. In fact, they have been known to exhibit complex behaviors such as cooperation, empathy, self-awareness, and even cultural traditions. For example, some elephant herds have been observed using sticks to retrieve food from distant locations, or creating specific \"routes\" through the savannah that are passed down from generation to generation.\n",
      "\n",
      "One particularly fascinating example of this is the \"great migration\" of the elephants in Botswana's Okavango Delta. Every year, thousands of elephants migrate over 30 miles (48 kilometers) to reach their summer grazing grounds, and they use a specific set of routes that have been etched into their brains through generations of travel.\n",
      "\n",
      "This level of cognitive ability is rare in the animal kingdom, and it's clear why scientists are so interested in studying elephant behavior and intelligence.\n"
     ]
    }
   ],
   "source": [
    "ask_llm(\"Tell me an interesting fact about elephants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASK LLM.....\n",
      "Here's an example of how you can use the Hugging Face Transformers library in Python to train a large language model like LLaMA:\n",
      "\n",
      "**Prerequisites**\n",
      "\n",
      "* Install the required libraries: `transformers`, `torch`, and `numpy`\n",
      "```bash\n",
      "pip install transformers torch numpy\n",
      "```\n",
      "\n",
      "**Code**\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import torch\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "from transformers import Trainer, TrainingArguments\n",
      "\n",
      "# Load the dataset (you can use a pre-trained model like LLaMA's base model)\n",
      "df = pd.read_csv('llama_train.csv')\n",
      "text = df['text']\n",
      "labels = df['label']\n",
      "\n",
      "# Preprocess the text data\n",
      "tokenizer = AutoTokenizer.from_pretrained('dipper-lamast/lamast-base')\n",
      "\n",
      "def preprocess(examples):\n",
      "    inputs = tokenizer(text, truncation=True, padding='max_length', max_length=512)\n",
      "    labels = [0 if label == 'neutral' else 1 for label in labels]\n",
      "    return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': torch.tensor(labels)}\n",
      "\n",
      "# Create a dataset class\n",
      "class LlamaDataset(torch.utils.data.Dataset):\n",
      "    def __init__(self, text, labels, tokenizer):\n",
      "        self.text = text\n",
      "        self.labels = labels\n",
      "        self.tokenizer = tokenizer\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        inputs = self.tokenizer(self.text[idx], truncation=True, padding='max_length', max_length=512)\n",
      "        return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': torch.tensor(self.labels[idx])}\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.text)\n",
      "\n",
      "# Create a dataset instance\n",
      "dataset = LlamaDataset(text, labels, tokenizer)\n",
      "\n",
      "# Define the training arguments\n",
      "training_args = TrainingArguments(\n",
      "    output_dir='./results',\n",
      "    num_train_epochs=3,\n",
      "    per_device_train_batch_size=16,\n",
      "    per_device_eval_batch_size=64,\n",
      "    warmup_steps=500,\n",
      "    weight_decay=0.01,\n",
      "    evaluate_during_training=True,\n",
      "    logging_dir='./logs'\n",
      ")\n",
      "\n",
      "# Create a model and tokenizer instance\n",
      "model = AutoModelForCausalLM.from_pretrained('dipper-lamast/lamast-base')\n",
      "tokenizer = AutoTokenizer.from_pretrained('dipper-lamast/lamast-base')\n",
      "\n",
      "# Initialize the trainer\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=dataset,\n",
      "    eval_dataset=dataset\n",
      ")\n",
      "\n",
      "# Train the model\n",
      "trainer.train()\n",
      "```\n",
      "\n",
      "**How it works**\n",
      "\n",
      "1. We load a dataset containing labeled text data.\n",
      "2. We preprocess the text data by tokenizing it using the `tokenizer`.\n",
      "3. We create a `Dataset` class that inherits from `torch.utils.data.Dataset`. This class contains methods for getting individual samples and getting the length of the dataset.\n",
      "4. We define training arguments using the `TrainingArguments` class from Hugging Face Transformers.\n",
      "5. We create a model instance and a tokenizer instance.\n",
      "6. We initialize the trainer with the model, training arguments, and datasets.\n",
      "7. We train the model using the `train()` method.\n",
      "\n",
      "**Note**: This code is just an example, and you may need to adjust it based on your specific use case. You can also experiment with different hyperparameters and models to improve performance.\n"
     ]
    }
   ],
   "source": [
    "ask_llm(\"how to write a  python code to train a ollama model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASK LLM.....\n",
      "LangChain-Chroma is a library for building text-to-text models, and OLLAMA (One-Layer Multi-Attention) is a neural network architecture designed specifically for this type of task. Here's an example Python code snippet to get you started with training a LangChain-Chroma model using OLLAMA:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from langchain.chromas import ChromaModel, TrainingArguments, Trainer\n",
      "from ollama import OLLAMAModel, OLLAMA Trainer\n",
      "\n",
      "# Set the device (GPU or CPU)\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "# Load pre-trained model weights\n",
      "model_name = 'ollama-base'\n",
      "weights_path = f'{model_name}.pyt'\n",
      "weights = torch.load(weights_path, map_location=device)\n",
      "\n",
      "# Create an instance of the ChromaModel with OLLAMA architecture\n",
      "chroma_model = ChromaModel(\n",
      "    num_layers=12,\n",
      "    embedding_dim=512,\n",
      "    hidden_dim=1024,\n",
      "    output_dim=512,\n",
      "    attention_heads=8,\n",
      "    dropout=0.1,\n",
      ")\n",
      "chroma_model.load_weights(weights)\n",
      "\n",
      "# Set up the training arguments\n",
      "training_args = TrainingArguments(\n",
      "    output_dir='./results',\n",
      "    num_train_epochs=3,\n",
      "    per_device_train_batch_size=16,\n",
      "    per_device_eval_batch_size=64,\n",
      "    warmup_steps=500,\n",
      "    weight_decay=0.01,\n",
      "    logging_dir='./logs',\n",
      ")\n",
      "\n",
      "# Create a trainer instance\n",
      "trainer = Trainer(chroma_model, training_args)\n",
      "\n",
      "# Load the dataset (you can use your own dataset or a pre-trained one)\n",
      "from transformers import AutoTokenizer\n",
      "tokenizer = AutoTokenizer.from_pretrained('longformer-base-4096')\n",
      "\n",
      "dataset_path = 'path_to_your_dataset'\n",
      "train_dataset = [line for line in open(dataset_path).readlines() if not line.startswith('#')]\n",
      "\n",
      "# Create an OLLAMATrainer instance\n",
      "ollama_trainer = OLLAMATrainer(\n",
      "    model=chroma_model,\n",
      "    device=device,\n",
      "    train_dataloader=train_dataset,\n",
      ")\n",
      "\n",
      "# Train the model\n",
      "ollama_trainer.train()\n",
      "```\n",
      "\n",
      "This code snippet assumes that you have the `langchain-chromas` and `ollama` libraries installed. You'll also need to replace `'path_to_your_dataset'` with the actual path to your dataset.\n",
      "\n",
      "Please note that this is just a starting point, and you may need to adjust the training arguments, model architecture, and other hyperparameters based on your specific use case and performance requirements.\n"
     ]
    }
   ],
   "source": [
    "ask_llm(\"how to write a python code using langchain_chroma and ollama to train a model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
